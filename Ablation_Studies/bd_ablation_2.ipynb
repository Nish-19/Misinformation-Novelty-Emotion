{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test embeddings\n",
    "import numpy as np\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# unpickling train_bd represenations\n",
    "train_novelty_dict = unpickle('Quora_Plain/train_bd_quora_plain.pickle')\n",
    "test_novelty_dict = unpickle('Quora_Plain/test_bd_quora_plain.pickle')\n",
    "\n",
    "# Converting to numpy arrays\n",
    "train_novelty_feature = np.stack(list(train_novelty_dict.values()))\n",
    "test_novelty_feature = np.stack(list(test_novelty_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape is (320552, 200)\n",
      "Test shape is (80126, 200)\n"
     ]
    }
   ],
   "source": [
    "# Examining the shape\n",
    "print('Train shape is', train_novelty_feature.shape)\n",
    "print('Test shape is', test_novelty_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'tid1', 'tid2', 'title1_zh', 'title2_zh', 'title1_en',\n",
      "       'title2_en', 'bd_label', 'Novelty_Labels', 'Emotion_1',\n",
      "       'Emotion_1_comb', 'Quora_Labels', 'Quora_Labels_new'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>bd_label</th>\n",
       "      <th>Novelty_Labels</th>\n",
       "      <th>Emotion_1</th>\n",
       "      <th>Emotion_1_comb</th>\n",
       "      <th>Quora_Labels</th>\n",
       "      <th>Quora_Labels_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>There are two new old-age insurance benefits f...</td>\n",
       "      <td>Police disprove \"bird's nest congress each per...</td>\n",
       "      <td>2</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>sadness</td>\n",
       "      <td>anger</td>\n",
       "      <td>novel</td>\n",
       "      <td>novel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "      <td>novel</td>\n",
       "      <td>novel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "      <td>novel</td>\n",
       "      <td>novel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP topped Hong Kong last year? She...</td>\n",
       "      <td>2</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "      <td>novel</td>\n",
       "      <td>novel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>\"用大蒜鉴别地沟油的方法,怎么鉴别地沟油</td>\n",
       "      <td>吃了30年食用油才知道，一片大蒜轻松鉴别地沟油</td>\n",
       "      <td>\"How to discriminate oil from gutter oil by me...</td>\n",
       "      <td>It took 30 years of cooking oil to know that o...</td>\n",
       "      <td>0</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>novel</td>\n",
       "      <td>duplicate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  tid1  tid2                          title1_zh  \\\n",
       "0   0     0     1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗   \n",
       "1   3     2     3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "2   1     2     4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "3   2     2     5  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "4   9     6     7               \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油   \n",
       "\n",
       "                    title2_zh  \\\n",
       "0    警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n",
       "1   深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n",
       "2        GDP首超香港？深圳澄清：还差一点点……   \n",
       "3  去年深圳GDP首超香港？深圳统计局辟谣：还差611亿   \n",
       "4     吃了30年食用油才知道，一片大蒜轻松鉴别地沟油   \n",
       "\n",
       "                                           title1_en  \\\n",
       "0  There are two new old-age insurance benefits f...   \n",
       "1  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "2  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "3  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "4  \"How to discriminate oil from gutter oil by me...   \n",
       "\n",
       "                                           title2_en  bd_label Novelty_Labels  \\\n",
       "0  Police disprove \"bird's nest congress each per...         2  contradiction   \n",
       "1  Shenzhen's GDP outstrips Hong Kong? Shenzhen S...         2        neutral   \n",
       "2  The GDP overtopped Hong Kong? Shenzhen clarifi...         2        neutral   \n",
       "3  Shenzhen's GDP topped Hong Kong last year? She...         2  contradiction   \n",
       "4  It took 30 years of cooking oil to know that o...         0  contradiction   \n",
       "\n",
       "  Emotion_1 Emotion_1_comb Quora_Labels Quora_Labels_new  \n",
       "0   sadness          anger        novel            novel  \n",
       "1     anger          anger        novel            novel  \n",
       "2     anger          anger        novel            novel  \n",
       "3     anger          anger        novel            novel  \n",
       "4   sadness        sadness        novel        duplicate  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO - Cateogorically encode the lables layer\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "train_df = pd.read_csv('../ByteDance_Dataset/train.csv')\n",
    "print(train_df.columns)\n",
    "le = LabelEncoder()\n",
    "train_df['bd_label'] = le.fit_transform(train_df['bd_label'])\n",
    "# train_df.label = train_df.label.astype('category').cat.codes #unrelated-2; disagreed-1; agreed-0\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'tid1', 'tid2', 'title1_zh', 'title2_zh', 'title1_en',\n",
      "       'title2_en', 'bd_label', 'Weight', 'Usage', 'Novelty_Labels',\n",
      "       'Emotion_1', 'Emotion_1_comb', 'Quora_Labels', 'Quora_Labels_new',\n",
      "       'Emotion_1_bdtrain', 'Quora_Labels_new_1'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>bd_label</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Usage</th>\n",
       "      <th>Novelty_Labels</th>\n",
       "      <th>Emotion_1</th>\n",
       "      <th>Emotion_1_comb</th>\n",
       "      <th>Quora_Labels</th>\n",
       "      <th>Quora_Labels_new</th>\n",
       "      <th>Emotion_1_bdtrain</th>\n",
       "      <th>Quora_Labels_new_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321187</td>\n",
       "      <td>167562</td>\n",
       "      <td>59521</td>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
       "      <td>egypt 's presidential election failed to win m...</td>\n",
       "      <td>Lyon! Lyon officials have denied that Felipe F...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>Private</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anger</td>\n",
       "      <td>disgust</td>\n",
       "      <td>novel</td>\n",
       "      <td>novel</td>\n",
       "      <td>anger</td>\n",
       "      <td>novel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321190</td>\n",
       "      <td>167564</td>\n",
       "      <td>91315</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>The Top 10 Americans believe that the Lizard M...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>Public</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "      <td>novel</td>\n",
       "      <td>duplicate</td>\n",
       "      <td>anger</td>\n",
       "      <td>duplicate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>321189</td>\n",
       "      <td>167563</td>\n",
       "      <td>167564</td>\n",
       "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>Will the United States wage war on Iraq withou...</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>Private</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anger</td>\n",
       "      <td>fear</td>\n",
       "      <td>novel</td>\n",
       "      <td>duplicate</td>\n",
       "      <td>sadness</td>\n",
       "      <td>duplicate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>321193</td>\n",
       "      <td>167564</td>\n",
       "      <td>160994</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>The hanging Saddam is a surrogate? This man's ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>Public</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>fear</td>\n",
       "      <td>anger</td>\n",
       "      <td>novel</td>\n",
       "      <td>duplicate</td>\n",
       "      <td>anger</td>\n",
       "      <td>duplicate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321191</td>\n",
       "      <td>167564</td>\n",
       "      <td>15084</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>中国川贝枇杷膏在美国受到热捧？纯属谣言！</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>Chinese loquat loquat plaster in America? Pure...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>Public</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>joy</td>\n",
       "      <td>anger</td>\n",
       "      <td>novel</td>\n",
       "      <td>duplicate</td>\n",
       "      <td>sadness</td>\n",
       "      <td>novel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    tid1    tid2                        title1_zh  \\\n",
       "0  321187  167562   59521  萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大   \n",
       "1  321190  167564   91315              萨达姆被捕后告诫美国的一句话，发人深思   \n",
       "2  321189  167563  167564    萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗   \n",
       "3  321193  167564  160994              萨达姆被捕后告诫美国的一句话，发人深思   \n",
       "4  321191  167564   15084              萨达姆被捕后告诫美国的一句话，发人深思   \n",
       "\n",
       "                     title2_zh  \\\n",
       "0  辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？   \n",
       "1    10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国   \n",
       "2          萨达姆被捕后告诫美国的一句话，发人深思   \n",
       "3  被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！   \n",
       "4         中国川贝枇杷膏在美国受到热捧？纯属谣言！   \n",
       "\n",
       "                                           title1_en  \\\n",
       "0  egypt 's presidential election failed to win m...   \n",
       "1  A message from Saddam Hussein after he was cap...   \n",
       "2  Will the United States wage war on Iraq withou...   \n",
       "3  A message from Saddam Hussein after he was cap...   \n",
       "4  A message from Saddam Hussein after he was cap...   \n",
       "\n",
       "                                           title2_en  bd_label  Weight  \\\n",
       "0  Lyon! Lyon officials have denied that Felipe F...         2  0.0625   \n",
       "1  The Top 10 Americans believe that the Lizard M...         2  0.0625   \n",
       "2  A message from Saddam Hussein after he was cap...         2  0.0625   \n",
       "3  The hanging Saddam is a surrogate? This man's ...         2  0.0625   \n",
       "4  Chinese loquat loquat plaster in America? Pure...         2  0.0625   \n",
       "\n",
       "     Usage Novelty_Labels Emotion_1 Emotion_1_comb Quora_Labels  \\\n",
       "0  Private        neutral     anger        disgust        novel   \n",
       "1   Public  contradiction     anger          anger        novel   \n",
       "2  Private        neutral     anger           fear        novel   \n",
       "3   Public  contradiction      fear          anger        novel   \n",
       "4   Public  contradiction       joy          anger        novel   \n",
       "\n",
       "  Quora_Labels_new Emotion_1_bdtrain Quora_Labels_new_1  \n",
       "0            novel             anger              novel  \n",
       "1        duplicate             anger          duplicate  \n",
       "2        duplicate           sadness          duplicate  \n",
       "3        duplicate             anger          duplicate  \n",
       "4        duplicate           sadness              novel  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the test set\n",
    "test_df = pd.read_csv('../ByteDance_Dataset/test_merged.csv')\n",
    "print(test_df.columns)\n",
    "#test_df.Category = test_df.Category.astype('category').cat.codes\n",
    "test_df['bd_label'] = le.transform(test_df['bd_label'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained Glove embeddings\n",
    "embeddings_dict = {}\n",
    "with open(\"../resources/glove.6B.200d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.45048   -0.1449     0.069873  -0.2611     0.51599    0.2649\n",
      " -0.37062   -0.4043    -0.047766  -0.85103    0.15076   -0.3983\n",
      " -0.30388    0.13779    0.31623   -0.27125    0.45444    0.93036\n",
      "  0.37158   -0.13067    0.016844   2.2395     0.21558   -0.97138\n",
      " -0.15899    0.5531    -0.045112  -0.76692    0.0094216 -0.12936\n",
      " -0.21059   -0.11888    0.1508    -0.2525    -0.22782   -0.53595\n",
      " -0.40099   -0.58793    0.059262  -0.64623   -0.20917   -0.03534\n",
      " -0.034241  -0.08936   -0.16375    0.36763    0.82737   -0.10209\n",
      "  0.19804    0.4031    -0.36257   -0.072119   0.2679    -0.20291\n",
      "  0.10427    0.24153   -0.06382    0.4669    -0.12288    0.11546\n",
      " -0.11928    0.12932   -0.8338    -0.82749    0.014886  -0.57084\n",
      " -0.58857   -0.089826   0.39842    0.022715   0.54963   -0.30594\n",
      "  0.058506   0.33867    0.17773    0.30477   -0.13231    0.1711\n",
      " -0.26207   -0.20595    0.58286   -0.090458   0.026476   0.090705\n",
      "  0.20869   -0.22999   -0.51844   -0.82133    0.81661   -0.63622\n",
      "  0.10171    1.0201     0.3413     0.26636   -0.18423    0.061951\n",
      " -0.40571   -0.090038  -0.41384   -0.20045   -0.19265    0.22574\n",
      "  0.36075    0.22961    0.17827   -0.46498   -0.12471    0.28899\n",
      " -0.62006   -0.065422  -0.39506   -0.2934    -0.27075   -0.12682\n",
      " -0.68685   -0.092482  -0.11464   -0.25478   -0.029841  -0.27236\n",
      " -0.23404   -0.084875  -0.25363   -0.23576   -0.088948  -0.39006\n",
      "  0.52574    0.27163    0.21287   -0.59627   -0.07769   -0.10208\n",
      "  0.48794    0.18492    0.42157   -0.12221   -0.13376    0.03495\n",
      " -0.24528    0.11253   -0.44813    0.22358    0.36009   -0.52455\n",
      "  0.95692   -0.1516     0.25071   -0.13371   -0.21225   -0.24028\n",
      " -0.0052063  0.23967    0.33641   -0.35559    0.32381    0.015276\n",
      " -0.72171    0.15128    0.075665   0.10067    0.80637   -0.1628\n",
      " -0.34519   -0.20409    0.044463   0.39453   -0.6072    -0.69917\n",
      " -0.0069373  0.65796   -0.068019   0.32129    0.057593  -0.37796\n",
      " -0.15254   -0.060348   0.032573  -0.073871  -0.4728     0.40693\n",
      "  0.65814   -0.20265    0.35995   -0.34156   -0.069076  -0.16736\n",
      "  0.27587   -0.32745    0.15163    0.4556    -0.083554   0.40252\n",
      "  0.041925  -0.41251    0.43337   -0.76944   -0.17903    0.0067027\n",
      " -0.084438   0.37511  ]\n",
      "['originally', 'latter', 'addition', 'same', 'feature']\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_dict[\"original\"])\n",
    "print(find_closest_embeddings(embeddings_dict[\"original\"])[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11542    0.62583    0.0031159 -0.27893    0.67254   -0.45319\n",
      " -0.16397   -0.32293   -0.26415   -0.25232   -0.52077   -0.088397\n",
      " -0.69601   -0.16587    0.12641   -0.60435   -0.45325    0.34664\n",
      " -0.37659   -0.44404   -0.54856    0.46296    0.16677    0.53835\n",
      " -0.51188   -0.026023   0.64821   -0.6264     0.44719   -0.15889\n",
      " -0.2688     0.035679   0.29732   -0.048158  -0.0096131 -0.37165\n",
      " -0.67745   -0.5086     0.64688    0.1884     0.19655    0.034364\n",
      "  0.29706    0.20052   -0.016906   0.0406     0.56526   -0.55097\n",
      " -0.10443   -0.22204   -0.03948   -0.90869   -0.14798    0.19678\n",
      " -0.15683    0.62182   -0.0029273 -0.4239    -0.063591  -0.12829\n",
      " -0.39695    0.21382   -0.10626    0.033134   0.5027    -0.35057\n",
      "  0.070328  -0.28327    0.29084    0.33744    0.56324   -0.46098\n",
      "  0.63119    0.21986   -0.73121    0.13872    0.0082016 -0.30085\n",
      " -0.25409    0.67545    0.36366    0.54803    0.68694    0.18578\n",
      "  0.29969   -0.55681   -0.41297   -0.3755     0.3044    -0.43858\n",
      "  0.36564    0.2197     0.29545    1.0777     0.16768   -0.23101\n",
      "  0.03877   -0.27676   -0.71461    0.13494   -0.060265  -0.11872\n",
      " -0.089161   0.10386    0.29261    1.0782     0.51453    0.23501\n",
      " -0.28589   -0.13676    0.20979    0.38332   -0.14897    0.28513\n",
      " -0.48382    0.62399    0.11771    0.26776    0.2938    -0.04101\n",
      "  0.094479   0.063694   0.2982    -0.90912   -0.94023    0.040108\n",
      "  0.056889   0.13672   -0.024812  -0.093205  -0.66293    0.45564\n",
      "  0.436     -0.24648   -0.28878    0.16454    0.32624    0.39854\n",
      "  0.17459    0.44784    0.35439    0.42704    0.22508    0.053823\n",
      " -0.20336    0.48407    0.015811   0.48895   -0.050272  -0.4788\n",
      " -0.42575    0.58736    0.052791  -0.59118   -0.75821    0.19537\n",
      "  0.33365    0.13213   -0.29306    0.24029    0.044808  -0.45017\n",
      "  0.5145    -0.53153   -0.33182    0.1463     0.26923    0.10841\n",
      " -0.19266    0.23607   -0.60267    0.042846  -0.3373    -0.76167\n",
      "  0.83496    0.18063   -0.073821  -0.46912   -0.44357    0.3812\n",
      "  0.44324    0.29473    0.70385   -0.32541    0.12961   -0.40899\n",
      "  0.31469    0.21163    0.3462     1.0456    -0.46367    0.074683\n",
      "  0.26506    0.068386   0.7965    -0.73647    0.15874    0.49409\n",
      "  0.39054    0.13027  ]\n",
      "['duplicates', 'duplicated', 'duplicating', 'replicate', 'flashaafp.com']\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_dict[\"duplicate\"])\n",
    "print(find_closest_embeddings(embeddings_dict[\"duplicate\"])[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# Storing for further use\n",
    "novel = embeddings_dict[\"original\"]\n",
    "duplicate = embeddings_dict[\"duplicate\"]\n",
    "print(novel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train bias (320552, 200)\n",
      "Test bias (80126, 200)\n"
     ]
    }
   ],
   "source": [
    "# Reading Labels info\n",
    "train_preds = pd.read_csv('Quora_Plain/train_bd_quora_plain.csv')\n",
    "test_preds = pd.read_csv('Quora_Plain/test_bd_quora_plain.csv')\n",
    "train_bias = []\n",
    "test_bias = []\n",
    "zero_vector = np.zeros((200,))\n",
    "for i, row in train_preds.iterrows():\n",
    "    if row['Label'] == 0:\n",
    "        train_bias.append(novel)\n",
    "    elif row['Label'] == 1:\n",
    "        train_bias.append(duplicate)\n",
    "    else:\n",
    "        train_bias.append(zero_vector)\n",
    "for i, row in test_preds.iterrows():\n",
    "    if row['Label'] == 0:\n",
    "        test_bias.append(novel)\n",
    "    elif row['Label'] == 1:\n",
    "        test_bias.append(duplicate)\n",
    "    else:\n",
    "        test_bias.append(zero_vector)\n",
    "train_bias = np.stack(train_bias)\n",
    "test_bias = np.stack(test_bias)\n",
    "print('Train bias', train_bias.shape)\n",
    "print('Test bias', test_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (320552, 200)\n",
      "Test (80126, 200)\n",
      "Max Train value 11.206837058067322\n",
      "Min Train value -1.141420766711235\n",
      "Max Test value 9.657366931438446\n",
      "Min Test value -1.1414207369089127\n"
     ]
    }
   ],
   "source": [
    "bd_train_nt_feature = np.add(train_novelty_feature, train_bias)\n",
    "bd_test_nt_feature = np.add(test_novelty_feature, test_bias)\n",
    "print('Train', bd_train_nt_feature.shape)\n",
    "print('Test', bd_test_nt_feature.shape)\n",
    "print('Max Train value', np.amax(bd_train_nt_feature))\n",
    "print('Min Train value', np.amin(bd_train_nt_feature))\n",
    "print('Max Test value', np.amax(bd_test_nt_feature))\n",
    "print('Min Test value', np.amin(bd_test_nt_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying simple logistic regression\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "# Without using the weight parameters\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bd_train_nt_feature = train_novelty_feature\n",
    "# bd_test_nt_feature = test_novelty_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (101239, 200)\n",
      "Train labels [0 0 0 ... 1 0 0]\n",
      "Test shape (28746, 200)\n",
      "Test labels [0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Removing the unrelated samples from both train and test\n",
    "#print(type(train_df['bd_label'] == 2))\n",
    "result = np.where(train_df['bd_label'] == 2)\n",
    "reduced_bd_nt_train = np.delete(bd_train_nt_feature, result[0], axis=0)\n",
    "print('Train shape', reduced_bd_nt_train.shape)\n",
    "reduced_train_labels = np.delete(train_df['bd_label'].values, result[0])\n",
    "print('Train labels', reduced_train_labels)\n",
    "result_test = np.where(test_df['bd_label']==2)\n",
    "reduced_bd_nt_test = np.delete(bd_test_nt_feature, result_test[0], axis=0)\n",
    "print('Test shape', reduced_bd_nt_test.shape)\n",
    "reduced_test_labels = np.delete(test_df['bd_label'].values, result_test[0])\n",
    "reduced_test_weights = np.delete(test_df['Weight'].values, result_test[0])\n",
    "print('Test labels', reduced_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train premise (101239, 768)\n",
      "Train hyp (101239, 768)\n",
      "Test premise (28746, 768)\n",
      "Test hyp (28746, 768)\n"
     ]
    }
   ],
   "source": [
    "# Considering the new emotion representations\n",
    "train_emotion_dict_pre = unpickle('../Proposed_Model/ByteDance_Data/train_ag_dg_premise.tsv_k_bal_bin.pickle')\n",
    "train_emotion_dict_hyp = unpickle('../Proposed_Model/ByteDance_Data/train_ag_dg_hyp.tsv_k_bal_bin.pickle')\n",
    "\n",
    "test_emotion_dict_pre = unpickle('../Proposed_Model/ByteDance_Data/test_ag_dg_premise.tsv_k_bal_bin.pickle')\n",
    "test_emotion_dict_hyp = unpickle('../Proposed_Model/ByteDance_Data/test_ag_dg_hyp.tsv_k_bal_bin.pickle')\n",
    "\n",
    "train_em_pre = np.stack(list(train_emotion_dict_pre.values()))\n",
    "train_em_hyp = np.stack(list(train_emotion_dict_hyp.values()))\n",
    "test_em_pre  = np.stack(list(test_emotion_dict_pre.values()))\n",
    "test_em_hyp  = np.stack(list(test_emotion_dict_hyp.values()))\n",
    "\n",
    "print('Train premise', train_em_pre.shape)\n",
    "print('Train hyp', train_em_hyp.shape)\n",
    "print('Test premise', test_em_pre.shape)\n",
    "print('Test hyp', test_em_hyp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (101239, 768)\n",
      "Test (28746, 768)\n",
      "Max Train value 1.9978583\n",
      "Min Train value -1.9980206\n",
      "Max Test value 1.9978583\n",
      "Min Test value -1.9980199\n"
     ]
    }
   ],
   "source": [
    "# Adding the premise and hypothesis\n",
    "train_em = np.add(train_em_pre, train_em_hyp)\n",
    "test_em = np.add(test_em_pre, test_em_hyp)\n",
    "print('Train', train_em.shape)\n",
    "print('Test', test_em.shape)\n",
    "print('Max Train value', np.amax(train_em))\n",
    "print('Min Train value', np.amin(train_em))\n",
    "print('Max Test value', np.amax(test_em))\n",
    "print('Min Test value', np.amin(test_em))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape is (101239, 200)\n",
      "Test shape is (28746, 200)\n"
     ]
    }
   ],
   "source": [
    "# Performing PCA to reduce the dimensions\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=200)\n",
    "train_em = pca.fit_transform(train_em)\n",
    "test_em = pca.transform(test_em)\n",
    "print('Train shape is', train_em.shape)\n",
    "print('Test shape is', test_em.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True emotion (200,)\n",
      "Fake emotion (200,)\n"
     ]
    }
   ],
   "source": [
    "# Word Embeddings\n",
    "# emotion_true = np.add(embeddings_dict['anticipation'], embeddings_dict['sadness'], embeddings_dict['joy'], embeddings_dict['trust'])\n",
    "# emotion_false = np.add(embeddings_dict['anger'], embeddings_dict['fear'], embeddings_dict['disgust'], embeddings_dict['surprise'])\n",
    "emotion_true = embeddings_dict['anticipation']+embeddings_dict['sadness']+embeddings_dict['joy']+embeddings_dict['trust']\n",
    "emotion_false = embeddings_dict['anger']+embeddings_dict['fear']+embeddings_dict['disgust']+embeddings_dict['surprise']\n",
    "print('True emotion', emotion_true.shape)\n",
    "print('Fake emotion', emotion_false.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ag_dg = pd.read_csv('../Proposed_Model/ByteDance_Data/train_ag_dg_only.csv')\n",
    "test_ag_dg = pd.read_csv('../Proposed_Model/ByteDance_Data/test_ag_dg_only.csv')\n",
    "train_hy_df = pd.read_csv('../Proposed_Model/ByteDance_Data/train_ag_dg_hyp.tsv_k_bal_numb_predictions_bin.csv')\n",
    "train_pre_df = pd.read_csv('../Proposed_Model/ByteDance_Data/train_ag_dg_premise.tsv_k_bal_numb_predictions_bin.csv')\n",
    "test_hy_df = pd.read_csv('../Proposed_Model/ByteDance_Data/test_ag_dg_hyp.tsv_k_bal_numb_predictions_bin.csv')\n",
    "test_pre_df = pd.read_csv('../Proposed_Model/ByteDance_Data/test_ag_dg_premise.tsv_k_bal_numb_predictions_bin.csv')\n",
    "assert len(train_ag_dg) == len(train_hy_df) == len(train_pre_df)\n",
    "assert len(test_ag_dg) == len(test_hy_df) == len(test_pre_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train bias (101239, 200)\n",
      "Test bias (28746, 200)\n"
     ]
    }
   ],
   "source": [
    "# New kind of adding scaffold labels\n",
    "train_bias_em = []\n",
    "test_bias_em = []\n",
    "zero_vector = np.zeros((200,))\n",
    "for i in range(len(train_ag_dg)):\n",
    "    pre = train_pre_df.loc[i, 'Emotion_Label']\n",
    "    hyp = train_hy_df.loc[i, 'Emotion_Label']\n",
    "    if train_ag_dg.loc[i, 'bd_label'] == 'agreed' and (pre==1 and hyp==1):\n",
    "        train_bias_em.append(emotion_false)\n",
    "    elif train_ag_dg.loc[i, 'bd_label'] == 'disagreed' and (pre==1 and hyp==0):\n",
    "        train_bias_em.append(emotion_true)\n",
    "    else:\n",
    "        train_bias_em.append(zero_vector)\n",
    "for i in range(len(test_ag_dg)):\n",
    "    pre = test_pre_df.loc[i, 'Emotion_Label']\n",
    "    hyp = test_hy_df.loc[i, 'Emotion_Label']\n",
    "    if test_ag_dg.loc[i, 'bd_label'] == 'agreed' and (pre==1 and hyp==1):\n",
    "        test_bias_em.append(emotion_false)\n",
    "    elif test_ag_dg.loc[i, 'bd_label'] == 'disagreed' and (pre==1 and hyp==0):\n",
    "        test_bias_em.append(emotion_true)\n",
    "    else:\n",
    "        test_bias_em.append(zero_vector)\n",
    "train_bias_em = np.stack(train_bias_em)\n",
    "test_bias_em = np.stack(test_bias_em)\n",
    "print('Train bias', train_bias_em.shape)\n",
    "print('Test bias', test_bias_em.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (101239, 200)\n",
      "Test (28746, 200)\n",
      "Max Train value 41.917335510253906\n",
      "Min Train value -36.551612854003906\n",
      "Max Test value 41.90174865722656\n",
      "Min Test value -36.551063537597656\n"
     ]
    }
   ],
   "source": [
    "bd_train_et_feature = np.add(train_em, train_bias_em)\n",
    "bd_test_et_feature = np.add(test_em, test_bias_em)\n",
    "print('Train', bd_train_et_feature.shape)\n",
    "print('Test', bd_test_et_feature.shape)\n",
    "print('Max Train value', np.amax(bd_train_et_feature))\n",
    "print('Min Train value', np.amin(bd_train_et_feature))\n",
    "print('Max Test value', np.amax(bd_test_et_feature))\n",
    "print('Min Test value', np.amin(bd_test_et_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Train (101239, 200)\n",
      "Combined Test (28746, 200)\n"
     ]
    }
   ],
   "source": [
    "combined_bd_train = np.add(reduced_bd_nt_train, bd_train_et_feature)\n",
    "combined_bd_test = np.add(reduced_bd_nt_test, bd_test_et_feature)\n",
    "print('Combined Train', combined_bd_train.shape)\n",
    "print('Combined Test', combined_bd_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=5000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Combined Logistic Regression Model\n",
    "lg_reg_combine = linear_model.LogisticRegression(max_iter = 5000)\n",
    "lg_reg_combine.fit(combined_bd_train, reduced_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Combined Reduced Logistic Regression model is: 91.735228155988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      agreed       0.91      1.00      0.95 1805.000009024593\n",
      "   disagreed       0.96      0.49      0.65 334.1999999999896\n",
      "\n",
      "    accuracy                           0.92 2139.200009024583\n",
      "   macro avg       0.94      0.74      0.80 2139.200009024583\n",
      "weighted avg       0.92      0.92      0.91 2139.200009024583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_combine = lg_reg_combine.predict(combined_bd_test)\n",
    "print(\"Accuracy of Combined Reduced Logistic Regression model is:\",\n",
    "#metrics.accuracy_score(test_df['Expected_bd'].values, y_pred, sample_weight = test_df['Weight'].values)*100)\n",
    "metrics.accuracy_score(reduced_test_labels, y_pred_combine, sample_weight = reduced_test_weights)*100)\n",
    "print(classification_report(reduced_test_labels, y_pred_combine, target_names = ['agreed', 'disagreed'], sample_weight = reduced_test_weights))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
