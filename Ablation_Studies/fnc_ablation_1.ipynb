{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49972, 1, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unpickling here\n",
    "dct = unpickle('ESIM_data/FNC/fnc_train_novelty.pickle')\n",
    "# Stacking the arrays to create a single feature matrix\n",
    "novelty_feature = np.stack(list(dct.values()))\n",
    "novelty_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49972, 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the middle dimension\n",
    "novelty_feature.resize((49972, 300))\n",
    "novelty_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25413, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the test set\n",
    "dct_test = unpickle('ESIM_data/FNC/fnc_test_novelty.pickle')\n",
    "test_feature = np.stack(list(dct_test.values()))\n",
    "test_feature.resize(list(test_feature.shape)[0], 300)\n",
    "test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Headline', 'Body ID', 'Stance', 'Body', 'Novelty_Labels', 'Emotion_1'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Body</th>\n",
       "      <th>Novelty_Labels</th>\n",
       "      <th>Emotion_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Police find mass graves with at least '15 bodi...</td>\n",
       "      <td>712</td>\n",
       "      <td>3</td>\n",
       "      <td>Danny Boyle is directing the untitled film\\r\\n...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>Hundreds of Palestinians were evacuated from t...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Christian Bale passes on role of Steve Jobs, a...</td>\n",
       "      <td>137</td>\n",
       "      <td>3</td>\n",
       "      <td>30-year-old Moscow resident was hospitalized w...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HBO and Apple in Talks for $15/Month Apple TV ...</td>\n",
       "      <td>1034</td>\n",
       "      <td>3</td>\n",
       "      <td>(Reuters) - A Canadian soldier was shot at the...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
       "      <td>1923</td>\n",
       "      <td>1</td>\n",
       "      <td>Fear not arachnophobes, the story of Bunbury's...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID  Stance  \\\n",
       "0  Police find mass graves with at least '15 bodi...      712       3   \n",
       "1  Hundreds of Palestinians flee floods in Gaza a...      158       0   \n",
       "2  Christian Bale passes on role of Steve Jobs, a...      137       3   \n",
       "3  HBO and Apple in Talks for $15/Month Apple TV ...     1034       3   \n",
       "4  Spider burrowed through tourist's stomach and ...     1923       1   \n",
       "\n",
       "                                                Body Novelty_Labels Emotion_1  \n",
       "0  Danny Boyle is directing the untitled film\\r\\n...  contradiction      fear  \n",
       "1  Hundreds of Palestinians were evacuated from t...     entailment   sadness  \n",
       "2  30-year-old Moscow resident was hospitalized w...        neutral   sadness  \n",
       "3  (Reuters) - A Canadian soldier was shot at the...  contradiction      fear  \n",
       "4  Fear not arachnophobes, the story of Bunbury's...        neutral      fear  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO - Cateogorically encode the lables layer\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "train_df = pd.read_csv('../FNC_Dataset/train_fnc_processed.csv')\n",
    "print(train_df.columns)\n",
    "le = LabelEncoder()\n",
    "train_df['Stance'] = le.fit_transform(train_df['Stance'])\n",
    "# train_df.label = train_df.label.astype('category').cat.codes #unrelated-2; disagreed-1; agreed-0\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Headline', 'Body ID', 'Stance', 'Body', 'Novelty_Labels', 'Emotion_1',\n",
      "       'Novelty_Quora', 'Novelty_Quora_1'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Body</th>\n",
       "      <th>Novelty_Labels</th>\n",
       "      <th>Emotion_1</th>\n",
       "      <th>Novelty_Quora</th>\n",
       "      <th>Novelty_Quora_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ferguson riots: Pregnant woman loses eye after...</td>\n",
       "      <td>2008</td>\n",
       "      <td>3</td>\n",
       "      <td>A RESPECTED senior French police officer inves...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crazy Conservatives Are Sure a Gitmo Detainee ...</td>\n",
       "      <td>1550</td>\n",
       "      <td>3</td>\n",
       "      <td>Dave Morin's social networking company Path is...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>fear</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Russian Guy Says His Justin Bieber Ringtone ...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>joy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zombie Cat: Buried Kitty Believed Dead, Meows ...</td>\n",
       "      <td>1793</td>\n",
       "      <td>3</td>\n",
       "      <td>Hewlett-Packard is officially splitting in two...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>joy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Argentina's President Adopts Boy to End Werewo...</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>An airline passenger headed to Dallas was remo...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>turst</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID  Stance  \\\n",
       "0  Ferguson riots: Pregnant woman loses eye after...     2008       3   \n",
       "1  Crazy Conservatives Are Sure a Gitmo Detainee ...     1550       3   \n",
       "2  A Russian Guy Says His Justin Bieber Ringtone ...        2       3   \n",
       "3  Zombie Cat: Buried Kitty Believed Dead, Meows ...     1793       3   \n",
       "4  Argentina's President Adopts Boy to End Werewo...       37       3   \n",
       "\n",
       "                                                Body Novelty_Labels Emotion_1  \\\n",
       "0  A RESPECTED senior French police officer inves...        neutral     anger   \n",
       "1  Dave Morin's social networking company Path is...        neutral      fear   \n",
       "2  A bereaved Afghan mother took revenge on the T...  contradiction       joy   \n",
       "3  Hewlett-Packard is officially splitting in two...     entailment       joy   \n",
       "4  An airline passenger headed to Dallas was remo...  contradiction     turst   \n",
       "\n",
       "   Novelty_Quora  Novelty_Quora_1  \n",
       "0              1                1  \n",
       "1              0                0  \n",
       "2              0                1  \n",
       "3              1                1  \n",
       "4              0                0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the test set\n",
    "test_df = pd.read_csv('../FNC_Dataset/competition_test_fnc_processed.csv')\n",
    "print(test_df.columns)\n",
    "#test_df.Category = test_df.Category.astype('category').cat.codes\n",
    "test_df['Stance'] = le.transform(test_df['Stance'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained Glove embeddings\n",
    "embeddings_dict = {}\n",
    "with open(\"../resources/glove.6B.200d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.45048   -0.1449     0.069873  -0.2611     0.51599    0.2649\n",
      " -0.37062   -0.4043    -0.047766  -0.85103    0.15076   -0.3983\n",
      " -0.30388    0.13779    0.31623   -0.27125    0.45444    0.93036\n",
      "  0.37158   -0.13067    0.016844   2.2395     0.21558   -0.97138\n",
      " -0.15899    0.5531    -0.045112  -0.76692    0.0094216 -0.12936\n",
      " -0.21059   -0.11888    0.1508    -0.2525    -0.22782   -0.53595\n",
      " -0.40099   -0.58793    0.059262  -0.64623   -0.20917   -0.03534\n",
      " -0.034241  -0.08936   -0.16375    0.36763    0.82737   -0.10209\n",
      "  0.19804    0.4031    -0.36257   -0.072119   0.2679    -0.20291\n",
      "  0.10427    0.24153   -0.06382    0.4669    -0.12288    0.11546\n",
      " -0.11928    0.12932   -0.8338    -0.82749    0.014886  -0.57084\n",
      " -0.58857   -0.089826   0.39842    0.022715   0.54963   -0.30594\n",
      "  0.058506   0.33867    0.17773    0.30477   -0.13231    0.1711\n",
      " -0.26207   -0.20595    0.58286   -0.090458   0.026476   0.090705\n",
      "  0.20869   -0.22999   -0.51844   -0.82133    0.81661   -0.63622\n",
      "  0.10171    1.0201     0.3413     0.26636   -0.18423    0.061951\n",
      " -0.40571   -0.090038  -0.41384   -0.20045   -0.19265    0.22574\n",
      "  0.36075    0.22961    0.17827   -0.46498   -0.12471    0.28899\n",
      " -0.62006   -0.065422  -0.39506   -0.2934    -0.27075   -0.12682\n",
      " -0.68685   -0.092482  -0.11464   -0.25478   -0.029841  -0.27236\n",
      " -0.23404   -0.084875  -0.25363   -0.23576   -0.088948  -0.39006\n",
      "  0.52574    0.27163    0.21287   -0.59627   -0.07769   -0.10208\n",
      "  0.48794    0.18492    0.42157   -0.12221   -0.13376    0.03495\n",
      " -0.24528    0.11253   -0.44813    0.22358    0.36009   -0.52455\n",
      "  0.95692   -0.1516     0.25071   -0.13371   -0.21225   -0.24028\n",
      " -0.0052063  0.23967    0.33641   -0.35559    0.32381    0.015276\n",
      " -0.72171    0.15128    0.075665   0.10067    0.80637   -0.1628\n",
      " -0.34519   -0.20409    0.044463   0.39453   -0.6072    -0.69917\n",
      " -0.0069373  0.65796   -0.068019   0.32129    0.057593  -0.37796\n",
      " -0.15254   -0.060348   0.032573  -0.073871  -0.4728     0.40693\n",
      "  0.65814   -0.20265    0.35995   -0.34156   -0.069076  -0.16736\n",
      "  0.27587   -0.32745    0.15163    0.4556    -0.083554   0.40252\n",
      "  0.041925  -0.41251    0.43337   -0.76944   -0.17903    0.0067027\n",
      " -0.084438   0.37511  ]\n",
      "['originally', 'latter', 'addition', 'same', 'feature']\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_dict[\"original\"])\n",
    "print(find_closest_embeddings(embeddings_dict[\"original\"])[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11542    0.62583    0.0031159 -0.27893    0.67254   -0.45319\n",
      " -0.16397   -0.32293   -0.26415   -0.25232   -0.52077   -0.088397\n",
      " -0.69601   -0.16587    0.12641   -0.60435   -0.45325    0.34664\n",
      " -0.37659   -0.44404   -0.54856    0.46296    0.16677    0.53835\n",
      " -0.51188   -0.026023   0.64821   -0.6264     0.44719   -0.15889\n",
      " -0.2688     0.035679   0.29732   -0.048158  -0.0096131 -0.37165\n",
      " -0.67745   -0.5086     0.64688    0.1884     0.19655    0.034364\n",
      "  0.29706    0.20052   -0.016906   0.0406     0.56526   -0.55097\n",
      " -0.10443   -0.22204   -0.03948   -0.90869   -0.14798    0.19678\n",
      " -0.15683    0.62182   -0.0029273 -0.4239    -0.063591  -0.12829\n",
      " -0.39695    0.21382   -0.10626    0.033134   0.5027    -0.35057\n",
      "  0.070328  -0.28327    0.29084    0.33744    0.56324   -0.46098\n",
      "  0.63119    0.21986   -0.73121    0.13872    0.0082016 -0.30085\n",
      " -0.25409    0.67545    0.36366    0.54803    0.68694    0.18578\n",
      "  0.29969   -0.55681   -0.41297   -0.3755     0.3044    -0.43858\n",
      "  0.36564    0.2197     0.29545    1.0777     0.16768   -0.23101\n",
      "  0.03877   -0.27676   -0.71461    0.13494   -0.060265  -0.11872\n",
      " -0.089161   0.10386    0.29261    1.0782     0.51453    0.23501\n",
      " -0.28589   -0.13676    0.20979    0.38332   -0.14897    0.28513\n",
      " -0.48382    0.62399    0.11771    0.26776    0.2938    -0.04101\n",
      "  0.094479   0.063694   0.2982    -0.90912   -0.94023    0.040108\n",
      "  0.056889   0.13672   -0.024812  -0.093205  -0.66293    0.45564\n",
      "  0.436     -0.24648   -0.28878    0.16454    0.32624    0.39854\n",
      "  0.17459    0.44784    0.35439    0.42704    0.22508    0.053823\n",
      " -0.20336    0.48407    0.015811   0.48895   -0.050272  -0.4788\n",
      " -0.42575    0.58736    0.052791  -0.59118   -0.75821    0.19537\n",
      "  0.33365    0.13213   -0.29306    0.24029    0.044808  -0.45017\n",
      "  0.5145    -0.53153   -0.33182    0.1463     0.26923    0.10841\n",
      " -0.19266    0.23607   -0.60267    0.042846  -0.3373    -0.76167\n",
      "  0.83496    0.18063   -0.073821  -0.46912   -0.44357    0.3812\n",
      "  0.44324    0.29473    0.70385   -0.32541    0.12961   -0.40899\n",
      "  0.31469    0.21163    0.3462     1.0456    -0.46367    0.074683\n",
      "  0.26506    0.068386   0.7965    -0.73647    0.15874    0.49409\n",
      "  0.39054    0.13027  ]\n",
      "['duplicates', 'duplicated', 'duplicating', 'replicate', 'flashaafp.com']\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_dict[\"duplicate\"])\n",
    "print(find_closest_embeddings(embeddings_dict[\"duplicate\"])[1:6])\n",
    "print(embeddings_dict[\"duplicate\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing for further use\n",
    "novel = embeddings_dict[\"original\"]\n",
    "duplicate = embeddings_dict[\"duplicate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train bias (49972, 200)\n",
      "Test bias (25413, 200)\n"
     ]
    }
   ],
   "source": [
    "# Reading Labels info\n",
    "train_bias = []\n",
    "test_bias = []\n",
    "zero_vector = np.zeros((200,))\n",
    "for i, row in train_df.iterrows():\n",
    "    if row['Novelty_Labels'] == 'contradiction' and row['Stance'] == 1:\n",
    "        train_bias.append(novel)\n",
    "    elif row['Novelty_Labels'] == 'entailment' and row['Stance'] == 0:\n",
    "        train_bias.append(duplicate)\n",
    "    else:\n",
    "        train_bias.append(zero_vector)\n",
    "for i, row in test_df.iterrows():\n",
    "    if row['Novelty_Labels'] == 'contradiction' and row['Stance'] == 1:\n",
    "        test_bias.append(novel)\n",
    "    elif row['Novelty_Labels'] == 'entailment' and row['Stance'] == 0:\n",
    "        test_bias.append(duplicate)\n",
    "    else:\n",
    "        test_bias.append(zero_vector)\n",
    "train_bias = np.stack(train_bias)\n",
    "test_bias = np.stack(test_bias)\n",
    "print('Train bias', train_bias.shape)\n",
    "print('Test bias', test_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape is (49972, 200)\n",
      "Test shape is (25413, 200)\n"
     ]
    }
   ],
   "source": [
    "# Performing PCA to reduce the dimensions\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=200)\n",
    "train_novelty_feature = pca.fit_transform(novelty_feature)\n",
    "test_novelty_feature = pca.transform(test_feature)\n",
    "print('Train shape is', train_novelty_feature.shape)\n",
    "print('Test shape is', test_novelty_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (49972, 200)\n",
      "Test (25413, 200)\n",
      "Max Train value 16.037691116333008\n",
      "Min Train value -11.610048800706863\n",
      "Max Test value 15.767217636108398\n",
      "Min Test value -11.282849311828613\n"
     ]
    }
   ],
   "source": [
    "fnc_train_nt_feature = np.add(train_novelty_feature, train_bias)\n",
    "fnc_test_nt_feature = np.add(test_novelty_feature, test_bias)\n",
    "print('Train', fnc_train_nt_feature.shape)\n",
    "print('Test', fnc_test_nt_feature.shape)\n",
    "print('Max Train value', np.amax(fnc_train_nt_feature))\n",
    "print('Min Train value', np.amin(fnc_train_nt_feature))\n",
    "print('Max Test value', np.amax(fnc_test_nt_feature))\n",
    "print('Min Test value', np.amin(fnc_test_nt_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8909,) (36545,)\n",
      "(45454,)\n",
      "Train shape (4518, 200)\n",
      "Train labels [0 1 0 ... 0 1 0]\n",
      "Test shape (2600, 200)\n",
      "Test labels [0 0 0 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Removing the unrelated samples from both train and test\n",
    "#print(type(train_df['bd_label'] == 2))\n",
    "result = np.where(train_df['Stance'] == 2)[0]\n",
    "result_1 = np.where(train_df['Stance'] == 3)[0]\n",
    "print(result.shape, result_1.shape)\n",
    "result_comb = np.concatenate((result, result_1))\n",
    "print(result_comb.shape)\n",
    "reduced_fnc_nt_train = np.delete(fnc_train_nt_feature, result_comb, axis=0)\n",
    "print('Train shape', reduced_fnc_nt_train.shape)\n",
    "reduced_train_labels = np.delete(train_df['Stance'].values, result_comb)\n",
    "print('Train labels', reduced_train_labels)\n",
    "result_test = np.where(test_df['Stance'] == 2)[0]\n",
    "result_test_1 = np.where(test_df['Stance'] == 3)[0]\n",
    "result_test_comb = np.concatenate((result_test, result_test_1))\n",
    "reduced_fnc_nt_test = np.delete(fnc_test_nt_feature, result_test_comb, axis=0)\n",
    "print('Test shape', reduced_fnc_nt_test.shape)\n",
    "reduced_test_labels = np.delete(test_df['Stance'].values, result_test_comb)\n",
    "print('Test labels', reduced_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6141924959216966, 1: 2.6892857142857145}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weight = compute_class_weight(class_weight='balanced', classes = np.unique(reduced_test_labels), y=reduced_train_labels)\n",
    "class_weight_dict = dict(enumerate(class_weight))\n",
    "print(class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=5000)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying simple logistic regression\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4518\n",
      "Train premise (4518, 768)\n",
      "Train hyp (4518, 768)\n",
      "Test premise (2600, 768)\n",
      "Test hyp (2600, 768)\n"
     ]
    }
   ],
   "source": [
    "# Load the train and test embeddings\n",
    "import numpy as np\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "# Considering the new emotion representations\n",
    "train_emotion_dict_pre = unpickle('../Proposed_Model/FNC_Dataset/train_ag_dg_premise_fnc.tsv_k_bal_bin.pickle')\n",
    "train_emotion_dict_hyp = unpickle('../Proposed_Model/FNC_Dataset/train_ag_dg_hyp_fnc.tsv_k_bal_bin.pickle')\n",
    "print(len(train_emotion_dict_pre))\n",
    "test_emotion_dict_pre = unpickle('../Proposed_Model/FNC_Dataset/test_ag_dg_premise_fnc.tsv_k_bal_bin.pickle')\n",
    "test_emotion_dict_hyp = unpickle('../Proposed_Model/FNC_Dataset/test_ag_dg_hyp_fnc.tsv_k_bal_bin.pickle')\n",
    "\n",
    "train_em_pre = np.stack(list(train_emotion_dict_pre.values()))\n",
    "train_em_hyp = np.stack(list(train_emotion_dict_hyp.values()))\n",
    "test_em_pre  = np.stack(list(test_emotion_dict_pre.values()))\n",
    "test_em_hyp  = np.stack(list(test_emotion_dict_hyp.values()))\n",
    "\n",
    "print('Train premise', train_em_pre.shape)\n",
    "print('Train hyp', train_em_hyp.shape)\n",
    "print('Test premise', test_em_pre.shape)\n",
    "print('Test hyp', test_em_hyp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (4518, 768)\n",
      "Test (2600, 768)\n",
      "Max Train value 1.9978582\n",
      "Min Train value -1.9980149\n",
      "Max Test value 1.9978582\n",
      "Min Test value -1.998014\n"
     ]
    }
   ],
   "source": [
    "# Adding the premise and hypothesis\n",
    "train_em = np.add(train_em_pre, train_em_hyp)\n",
    "test_em = np.add(test_em_pre, test_em_hyp)\n",
    "print('Train', train_em.shape)\n",
    "print('Test', test_em.shape)\n",
    "print('Max Train value', np.amax(train_em))\n",
    "print('Min Train value', np.amin(train_em))\n",
    "print('Max Test value', np.amax(test_em))\n",
    "print('Min Test value', np.amin(test_em))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape is (4518, 200)\n",
      "Test shape is (2600, 200)\n"
     ]
    }
   ],
   "source": [
    "# Performing PCA to reduce the dimensions\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=200)\n",
    "train_em = pca.fit_transform(train_em)\n",
    "test_em = pca.transform(test_em)\n",
    "print('Train shape is', train_em.shape)\n",
    "print('Test shape is', test_em.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True emotion (200,)\n",
      "Fake emotion (200,)\n"
     ]
    }
   ],
   "source": [
    "# Word Embeddings\n",
    "# emotion_true = np.add(embeddings_dict['anticipation'], embeddings_dict['sadness'], embeddings_dict['joy'], embeddings_dict['trust'])\n",
    "# emotion_false = np.add(embeddings_dict['anger'], embeddings_dict['fear'], embeddings_dict['disgust'], embeddings_dict['surprise'])\n",
    "emotion_true = embeddings_dict['anticipation']+embeddings_dict['sadness']+embeddings_dict['joy']+embeddings_dict['trust']\n",
    "emotion_false = embeddings_dict['anger']+embeddings_dict['fear']+embeddings_dict['disgust']+embeddings_dict['surprise']\n",
    "print('True emotion', emotion_true.shape)\n",
    "print('Fake emotion', emotion_false.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ag_dg = pd.read_csv('../Proposed_Model/FNC_Data/train_ag_dg_only_fnc.csv')\n",
    "test_ag_dg = pd.read_csv('../Proposed_Model/FNC_Data/test_ag_dg_only_fnc.csv')\n",
    "train_hy_df = pd.read_csv('../Proposed_Model/FNC_Data/train_ag_dg_hyp_fnc.tsv_k_bal_numb_predictions_bin.csv')\n",
    "train_pre_df = pd.read_csv('../Proposed_Model/FNC_Data/train_ag_dg_premise_fnc.tsv_k_bal_numb_predictions_bin.csv')\n",
    "test_hy_df = pd.read_csv('../Proposed_Model/FNC_Data/test_ag_dg_hyp_fnc.tsv_k_bal_numb_predictions_bin.csv')\n",
    "test_pre_df = pd.read_csv('../Proposed_Model/FNC_Data/test_ag_dg_premise_fnc.tsv_k_bal_numb_predictions_bin.csv')\n",
    "assert len(train_ag_dg) == len(train_hy_df) == len(train_pre_df)\n",
    "assert len(test_ag_dg) == len(test_hy_df) == len(test_pre_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train bias (4518, 200)\n",
      "Test bias (2600, 200)\n"
     ]
    }
   ],
   "source": [
    "# New kind of adding scaffold labels\n",
    "train_bias_em = []\n",
    "test_bias_em = []\n",
    "zero_vector = np.zeros((200,))\n",
    "for i in range(len(train_ag_dg)):\n",
    "    if train_ag_dg.loc[i, 'Stance'] == 'agree' and train_pre_df.loc[i, 'Emotion_Label'] == 0 and train_hy_df.loc[i, 'Emotion_Label'] == 0:\n",
    "        train_bias_em.append(emotion_true)\n",
    "    elif train_ag_dg.loc[i, 'Stance'] == 'disagree' and train_pre_df.loc[i, 'Emotion_Label'] == 0 and train_hy_df.loc[i, 'Emotion_Label'] == 1:\n",
    "        train_bias_em.append(emotion_false)\n",
    "    else:\n",
    "        train_bias_em.append(zero_vector)\n",
    "for i in range(len(test_ag_dg)):\n",
    "    if test_ag_dg.loc[i, 'Stance'] == 'agree' and test_pre_df.loc[i, 'Emotion_Label'] == 0 and test_hy_df.loc[i, 'Emotion_Label'] == 0:\n",
    "        test_bias_em.append(emotion_true)\n",
    "    elif test_ag_dg.loc[i, 'Stance'] == 'disagree' and test_pre_df.loc[i, 'Emotion_Label'] == 0 and test_hy_df.loc[i, 'Emotion_Label'] == 1:\n",
    "        test_bias_em.append(emotion_false)\n",
    "    else:\n",
    "        test_bias_em.append(zero_vector)\n",
    "train_bias_em = np.stack(train_bias_em)\n",
    "test_bias_em = np.stack(test_bias_em)\n",
    "print('Train bias', train_bias_em.shape)\n",
    "print('Test bias', test_bias_em.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (4518, 200)\n",
      "Test (2600, 200)\n",
      "Max Train value 46.74790608882904\n",
      "Min Train value -32.49567794799805\n",
      "Max Test value 46.748016715049744\n",
      "Min Test value -32.49523162841797\n"
     ]
    }
   ],
   "source": [
    "fnc_train_et_feature = np.add(train_em, train_bias_em)\n",
    "fnc_test_et_feature = np.add(test_em, test_bias_em)\n",
    "print('Train', fnc_train_et_feature.shape)\n",
    "print('Test', fnc_test_et_feature.shape)\n",
    "print('Max Train value', np.amax(fnc_train_et_feature))\n",
    "print('Min Train value', np.amin(fnc_train_et_feature))\n",
    "print('Max Test value', np.amax(fnc_test_et_feature))\n",
    "print('Min Test value', np.amin(fnc_test_et_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Train (4518, 200)\n",
      "Combined Test (2600, 200)\n"
     ]
    }
   ],
   "source": [
    "combined_fnc_train = np.add(reduced_fnc_nt_train, fnc_train_et_feature)\n",
    "combined_fnc_test = np.add(reduced_fnc_nt_test, fnc_test_et_feature)\n",
    "print('Combined Train', combined_fnc_train.shape)\n",
    "print('Combined Test', combined_fnc_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=5000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Combined Logistic Regression Model\n",
    "lg_reg_combine = linear_model.LogisticRegression(max_iter = 5000)\n",
    "lg_reg_combine.fit(combined_fnc_train, reduced_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Combined Reduced Logistic Regression model is: 88.1923076923077\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      agreed       0.89      0.96      0.92      1903\n",
      "   disagreed       0.87      0.66      0.75       697\n",
      "\n",
      "    accuracy                           0.88      2600\n",
      "   macro avg       0.88      0.81      0.84      2600\n",
      "weighted avg       0.88      0.88      0.88      2600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_combine = lg_reg_combine.predict(combined_fnc_test)\n",
    "print(\"Accuracy of Combined Reduced Logistic Regression model is:\",\n",
    "metrics.accuracy_score(reduced_test_labels, y_pred_combine)*100)\n",
    "print(classification_report(reduced_test_labels, y_pred_combine, target_names = ['agreed', 'disagreed']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
